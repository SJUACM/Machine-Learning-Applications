{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"![](https://cdn.geekwire.com/wp-content/uploads/2020/03/bigstock-Residential-Neighborhood-And-F-345221452-630x420.jpg)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Hi and welcome to this notebook on predicting house prices! If you are new to data analysis, Python and/or machine learning, this is the perfect place to start!\n\nThis notebook will guide you through this dataset - it is composed of more than 4,000 houses and the price they sold for, as well as some of their features, like number of bedrooms, bathrooms, etc...\n\nThe goal of this notebook is to answer the following question: *what are the most important features that influence the price of a house?*\n\nHere is what we're going to do:\n\n1. Exploratory Data Analysis (EDA) - let's find out what the features are and how they individually affect house prices\n2. Recoding - fixing the problems we've identified in EDA\n3. Feature selection - we have to be picky about which features we include in the model - which ones have to be in and which ones are optional?\n4. Model building - let's see what is the impact of our features on house prices!\n\nSo if you haven't already, grab yourself a nice cuppa and let's dig in!\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# here are the modules we'll be using throughout this notebook\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport math","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"from scipy import stats","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Load our data from the csv file\nhouses = pd.read_csv('../input/housedata/data.csv') ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, how many houses do we have in our dataset?","execution_count":null},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"houses.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, so the first number tells us the number of rows (the number of houses) and the second one is the number of columns (the number of features).\n\nWe have 4600 houses in the dataset and 18 features, including price. Therefore, we can choose between 17 different features that influence the price of a house. Let's check these out.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"houses.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a nice list of all the features, some being *categorical variables* (object types), like the country of the house and some being *measures* (float64 or int64 types) like the surface of the basement.\n\nJust curious, what country are these houses in?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"houses.country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interesting. What state ?","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"houses.statezip.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright, so all of the houses are located in the state of Washington (Pacific Northwest, where Seattle is). This might serve us for later.\n\nLet's check out the average price of a house in that area.","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\"The average price of a house is ${:,.0f}\".format(houses.price.mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I live in New Zealand and **550,000 USD** is roughly **900,000 NZD**, which gets you quite a nice house down here!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"To me the first feature of a house that stands out is the number of bedrooms. Bigger houses have more bedrooms and thus command a higher price. Let's look at that relationship.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the average price for houses along their number of bedrooms:\nplt.figure(figsize=(10,6))\nsns.barplot(x=houses.bedrooms, y=houses['price'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK something strange here.  There is clearly a relationship between the number of bedrooms and the average price of a house. However, seems that a house with 9 bedrooms (!) sells for less than a house with 4 bedrooms... \n\nAlso, some houses don't have any rooms?\n\nLet's look at this in more detail.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# get a price breakdown for each bedroom group\nbybedroom = houses.groupby(['bedrooms']).price.agg([len, min, max])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#problem #1 and #2 - 2 houses with 0 bedrooms, giant outlier at 3 bedrooms\nbybedroom","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The table above provides an explanation for the price discrepancy we have seen. There is only one house with 9 bedrooms! This may be a house located far from the city, or the owner might have needed to sell it in a hurry. Whatever the circumstances, 1 house is not big enough a sample. We'll need to do something if we want to use the number of bedrooms as a predictor in our model.\n\nThe table above also highlighted 2 other problems with the data. \n1. Two houses have no bedroom!\n2. Some houses have a price of zero\n\nLet's look at this last problem in more detail.","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# problem #3 - houses with null prices\nhouses_zero= houses[houses.price==0]\nprint('There are '+str(len(houses_zero))+' houses without a price')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Out of 4600 houses in the sample, 49 don't have a price. It's not a lot, but this might confuse the model.\n\nWe're almost done with exploring the features. Let's look at the price distribution.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# problem #4 - house prices are not normal\nsns.distplot(houses['price'], fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The price distribution is in blue, while the normal distribution is in black. Clearly, houses prices are not normal. This is not a problem per se, rather something to keep in mind.\n\nSo, to recap, we have 3 problems :\n1. Houses with 0 bedroom\n2. Giant outlier at almost $27M - 50 times the price of a normal house\n3. 49 houses without a price\n\n\nWe'll take the easy way out - remove them from our analysis.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# new dataframe without problem #1 #2 #3\nhouses_o = houses[(houses.price<2.5*10**7) & (houses.bedrooms>0) & (houses.price>0)].copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, there is one other potential problem with our data. There are too few houses with more than 6 bedrooms. This is a problem if we want to use the number of bedrooms as a predictor of house price.\n\nTo fix this, we can simply group the houses with 7, 8 and 9 bedrooms with the houses featuring 6 bedrooms.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#recode houses with more than 6 bedrooms as 6 bedrooms\nhouses_o['bedrooms_recoded'] = houses_o['bedrooms'].replace([7,8,9],6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"houses_o['renovated_0_1'] = houses_o['yr_renovated']/houses_o['yr_renovated']\nhouses_o['renovated_0_1'] = houses_o['renovated_0_1'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK we're done with the recoding.\nLet's get a nice Pearson correlation matrix going on","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"features = ['price','bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot',\n       'floors', 'waterfront', 'view', 'condition', 'sqft_above',\n       'sqft_basement', 'yr_built', 'yr_renovated']\nmask = np.zeros_like(houses_o[features].corr(), dtype=np.bool) \nmask[np.triu_indices_from(mask)] = True \n\nf, ax = plt.subplots(figsize=(16, 12))\nplt.title('Pearson Correlation Matrix',fontsize=25)\n\nsns.heatmap(houses_o[features].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"BuGn\", #\"BuGn_r\" to reverse \n            linecolor='w',annot=True,annot_kws={\"size\":8},mask=mask,cbar_kws={\"shrink\": .9});","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, this can be a bit overwhelming, so let's focus on one element at a time. \n\nThe first thing to look at is the first column. It tells us how correlated the features in the houses are to the house price.\nWe can see that the most correlated feature is sqft_living, with a coefficent of 0.62. This makes sense - the higher the surface of the house, the higher the price.\n\nThe second most correlated feature is sqft_above, with 0.53. However, in our model **we cannot use both sqft_living and sqft_above**, and that's because these features are highly correlated - 0.88.\n\nIf we do use both of these features, our model won't be able to properly estimate the coefficients - it won't know whether the price is high because sqft_living is high or because sqft_above is high.\n\nSo, best practice is to select features that are highly correlated with house prices, but not correlated with each other.\nFor now, we'll pick the following:\n\n1. bedrooms_recoded\n2. floors\n3. view\n4. condition\n5. renovated_0_1","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's go ahead and separate the price from our features:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Move our features into the X DataFrame\nX = houses_o.loc[:,['bedrooms_recoded', 'floors','view','condition','renovated_0_1']]\n\n# Move our labels into the y DataFrame\ny = houses_o.loc[:,['price']] ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we need to separate our houses into train and test set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate y and X into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X, \n                                                    y, \n                                                    test_size=0.3, \n                                                    random_state=42\n                                                   )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finally, we use a multiple regression model on the train set to find out what is the impact of our predictor variables on price:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#train a basic multiple regression model and print out the coefficients\nmod = sm.OLS(y_train, X_train)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We get a nice little novel above. I know this is a lot but it's necessary. \n\nWe can see for each of our predictor variables, (*bedrooms_recoded, floors, view,condition, renovated_0_1*) there are several columns - *coef, std err, t,  P>|t|, [0.025 and 0.975]*. The column we need to check first is *P>|t|*. It tells us what is the probability that our coefficients are equal to zero, meaning our predictor variables do not have an impact on price.\n\nHere we're lucky - most of these probabilities are zero, except for the condition of the house and whether it has been renovated or not.\n\nThe probability that *condition* and *renovated_0_1* are zero is 12.1% and 59.9%, respectively.\n\nThe next thing we can check is the coefficients themselves - what is the most important predictor variable? Here it seems that it's *view*, followed closely by *floors*. According to the model, if the house has a view, it will gain about $ 170k in value (1.712e+05 = 1,712*(10^5))\n\nSo this is obviously not a great model, but let's see what it does right and what it does wrong.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ask the model to predict prices in the train and test set based just on our predictor variables\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ntest_pre = lr.predict(X_test)\ntrain_pre = lr.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now let's plot our predicted values on one axis and the real values on the other axis\nplt.scatter(train_pre, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre, y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper right\")\nplt.plot([0.2*10**6, 0.25*10**7], [0.2*10**6, 0.25*10**7], c = \"red\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The x-axis represents the prices predicted by the model, while the y-axis shows the true price of these houses. Ideally, we would want houses to be grouped on the red line, meaning the estimated value and the true value of a house are very close. \n\nWe are not there yet, but it isn't too bad! Our predictions follow the red line and there is no obvious train/test bias. Also, it seems that our predictions don't get worse when house prices increase, which is a good sign.\n\nIt seems our model has trouble with high-value houses though. It accurately forecasts the price of cheap and moderately priced properties, but when prices rise on the y axis, this is where we start to deviate from the ideal red line. The poster child for this prediction error is the obvious outlier sitting at 1.2x10^7 = a 12 million dollar house, where our model predicted less than $ 500 000!\n\nNow let's compute the mean error.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the results from the regression in dataframe format\nres = pd.DataFrame(data=train_pre, columns=['predicted values'])\n#join with the actual prices\nres = y_train.reset_index().join(res)\n#join with the training dataset\nresfin = res.join(X_train, on='index',lsuffix='_y')\n# compute the actual prices, predicted prices and error\nresfin['predprice']=res['predicted values']\nresfin['actprice']=res['price']\nresfin['error']=resfin['predprice']-resfin['actprice']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the results from the regression in dataframe format\nres_test = pd.DataFrame(data=test_pre, columns=['predicted values'])\n#join with the actual prices\nres_test = y_test.reset_index().join(res_test)\n#join with the training dataset\nresfin_test = res_test.join(X_test, on='index',lsuffix='_y')\n# compute the actual prices, predicted prices and error\nresfin_test['predprice']=resfin_test['predicted values']\nresfin_test['actprice']=resfin_test['price']\nresfin_test['error']=resfin_test['predprice']-resfin_test['actprice']\nresdf = pd.concat([resfin,resfin_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"The mean error of our model is ${:,.0f}\".format(resfin_test['error'].mean())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Mean error on the test set is close to  $ 14k, which means that the model tends to overestimate the value of houses.\n\nLet's see what the shape of the errors looks like.\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the error\nplt.figure(figsize=(15,8))\nsns.distplot(resfin_test['error'], fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next let's isolate the model's biggest mistakes to see if there is a pattern.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardize the errors\nx_array = np.array(resfin_test['error'])\nnormalized_X = stats.zscore(x_array)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's get the normalized error back into our dataset\nerror_df = pd.DataFrame(data=normalized_X.T, columns=['normalized error'])\nresfin2 = resfin_test.join(error_df)\nresfin2['abs_norm_error'] = abs(resfin2['normalized error'])\n#now let's select only the errors that are 2 standard deviations away from the mean\nresfin2['massive underestimation'] = resfin2['normalized error']<-2 \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.distplot(error_df, fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Now that we have flagged our biggest mistakes, it will be easier to find out whether there is a pattern to them.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#how many big mistakes in our test dataset?\nresfin2['massive underestimation'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"50 houses are underestimated in our test data! How much is that in %?","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"\"approximately {:.1%} of the test houses are massively underestimated\".format(resfin2['massive underestimation'].values.sum()/len(resfin2))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(resfin2['predprice'], resfin2['actprice'], c = resfin2['massive underestimation'])\nplt.plot([0.2*10**6, 1.75*10**6], [0.2*10**6, 1.75*10**6], c = \"red\")\nplt.legend(loc = \"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! We've highlighted the underestimated houses in yellow. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Now let's explore - what kind of houses is the model particularly bad at estimating the price of?\npd.crosstab(resfin2['bedrooms_recoded'],resfin2['massive underestimation']).apply(lambda r: r/r.sum(), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK so the model apparently has trouble with houses that feature 4 rooms or more. There may be something we're missing about these houses. Could be the location? After all, according to real estate agents, it's all about location!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Let's visualise the price of a house and the zip code!","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"result = houses_o.groupby([\"statezip\"])['price'].aggregate(np.median).reset_index().sort_values('price', ascending=False)\nplt.figure(figsize=(15,8))\nchart = sns.barplot(\n    x='statezip',\n    y='price',\n    data=houses_o,\n    order = result['statezip'],\n    estimator=np.median\n    \n    \n)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We can see there are a handful of zipcodes that are way more expensive than the rest. Otherwise, statezip doesn't seem to play a huge role in the price of a house.\n\nLet's select the 5 most expensive zips and create a new binary variable called 'posh zip':","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"houses_o['posh_zip'] = houses_o['statezip'].isin(['WA 98039','WA 98004','WA 98040','WA 98109']).astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Move our features into the X DataFrame\nX = houses_o.loc[:,['bedrooms_recoded', 'floors', 'condition','view','renovated_0_1', 'posh_zip']]\n\n# Move our labels into the y DataFrame\ny = houses_o.loc[:,['price']] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate y and X into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X, \n                                                    y, \n                                                    test_size=0.3, \n                                                    random_state=42\n                                                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train a basic multiple regression model and print out the coefficients\nmod = sm.OLS(y_train, X_train)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"OK, let's unpack this shall we?\n\nWhat's reassuring is that condition and renovated_0_1 stay non significant, meaning they don't seem to contribute too much to the price of a house. However posh_zip does contribute and not in a small way! It is the most significant variable by a long shot AND it contributes the most to the price of a house! If you could move a house from an average neighborhood to a good neighborhood, it increases the house value by $ 700k, everything else being constant!\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ask the model to predict prices in the train and test set based just on our predictor variables\nlr = LinearRegression()\nlr.fit(X_train,y_train)\ntest_pre = lr.predict(X_test)\ntrain_pre = lr.predict(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#get the results from the regression in dataframe format\nres_test = pd.DataFrame(data=test_pre, columns=['predicted values'])\n#join with the actual prices\nres_test = y_test.reset_index().join(res_test)\n#join with the training dataset\nresfin_test = res_test.join(X_test, on='index',lsuffix='_y')\n# compute the actual prices, predicted prices and error\nresfin_test['predprice']=resfin_test['predicted values']\nresfin_test['actprice']=resfin_test['price']\nresfin_test['error']=resfin_test['predprice']-resfin_test['actprice']\nresdf = pd.concat([resfin,resfin_test])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardize the errors\nx_array = np.array(resfin_test['error'])\nnormalized_X = stats.zscore(x_array)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#let's get the normalized error back into our dataset\nerror_df = pd.DataFrame(data=normalized_X.T, columns=['normalized error'])\nresfin2 = resfin_test.join(error_df)\nresfin2['abs_norm_error'] = abs(resfin2['normalized error'])\n#now let's select only the errors that are 2 standard deviations away from the mean\nresfin2['massive underestimation'] = resfin2['normalized error']<-2 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.scatter(resfin2['predprice'], resfin2['actprice'], c = resfin2['massive underestimation'])\nplt.plot([0.2*10**6, 1.75*10**6], [0.2*10**6, 1.75*10**6], c = \"red\")\nplt.legend(loc = \"upper left\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Our model has one too many variables, but it could me missing something too. The Omnibus test tells us about the probability that the residuals are not normally distributed. The result of this test was zero which means they're not. If they're not normal, it means there is a pattern to the residuals that we're missing, and maybe this pattern could be explained by another variable in our dataset.\n\nFor now, let's just check the shape of our residuals:","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the residuals\nplt.figure(figsize=(15,8))\nsns.distplot(res.resid, fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Welp, something is definitely off here. there is a first peak around -1, and then we get the main peak around 1! That is surely not normal.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Move our features into the X DataFrame\nX = houses_o.loc[:,['sqft_living','condition', 'yr_built']]\n\n# Move our labels into the y DataFrame\ny = houses_o.loc[:,['price']] ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# separate y and X into train and test\nX_train, X_test, y_train, y_test = train_test_split(\n                                                    X, \n                                                    y, \n                                                    test_size=0.3, \n                                                    random_state=42\n                                                   )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train a basic multiple regression model and print out the coefficients\nmod = sm.OLS(y_train, X_train)\nres = mod.fit()\nprint(res.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot the residuals\nplt.figure(figsize=(15,8))\nsns.distplot(res.resid, fit=norm)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#partial regression plots\nfig = plt.figure(figsize=(12,8))\nfig = sm.graphics.plot_partregress_grid(res, fig=fig)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}